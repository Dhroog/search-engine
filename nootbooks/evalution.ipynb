{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rFA7mS-c8LWM"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def lowercase_filter(tokens):\n",
        "    return [token.lower() for token in tokens]\n",
        "\n",
        "\n",
        "def stem_filter(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "\n",
        "def punctuation_filter(tokens):\n",
        "    return [PUNCTUATION.sub('', token) for token in tokens]\n",
        "\n",
        "\n",
        "# top 25 most common words in English and \"wikipedia\":\n",
        "# https://en.wikipedia.org/wiki/Most_common_words_in_English\n",
        "STOPWORDS = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n",
        "                 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n",
        "                 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'wikipedia'])\n",
        "\n",
        "\n",
        "def stopword_filter(tokens):\n",
        "    return [token for token in tokens if token not in STOPWORDS]\n",
        "\n",
        "\n",
        "def analyze(text):\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = lowercase_filter(tokens)\n",
        "    tokens = punctuation_filter(tokens)\n",
        "    tokens = stopword_filter(tokens)\n",
        "    # tokens = stem_filter(tokens)\n",
        "\n",
        "    return [token for token in tokens if token]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X07x9S7t8ikB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self, path_docs, path_queries, path_grels):\n",
        "        self.docs = pd.read_csv(path_docs)\n",
        "        self.queries = pd.read_csv(path_queries)\n",
        "        self.qrels = pd.read_csv(path_grels)\n",
        "\n",
        "    def set_docs(self, path):\n",
        "        self.docs = pd.read_csv(path)\n",
        "\n",
        "    def set_queries(self, path):\n",
        "        self.queries = pd.read_csv(path)\n",
        "\n",
        "    def set_qrels(self, path):\n",
        "        self.qrels = pd.read_csv(path)\n",
        "\n",
        "    def analyez_docs(self, name_col):\n",
        "        self.docs['analyzed_tokens'] = self.docs['content'].apply(analyze)\n",
        "        self.docs[name_col] = self.docs['analyzed_tokens'].apply(\n",
        "            lambda tokens: \" \".join(tokens))\n",
        "\n",
        "    def analyez_queries(self, name_col):\n",
        "        self.queries['analyzed_tokens'] = self.queries['text'].apply(analyze)\n",
        "        self.queries[name_col] = self.queries['analyzed_tokens'].apply(\n",
        "            lambda tokens: \" \".join(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QI9VCFws8zl4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "\n",
        "class TFIDF:\n",
        "    def __init__(self, min_df, max_df):\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english',  # ngram_range=(2, 4),\n",
        "                                     min_df=min_df,\n",
        "                                     max_df=max_df)\n",
        "        self.scaler = MaxAbsScaler()\n",
        "        self.matrix = None\n",
        "        self.queries_vec = None\n",
        "\n",
        "    def init_tf_idf_matrix(self, docs, name_col):\n",
        "        self.matrix = self.tfidf.fit_transform(docs[name_col])\n",
        "\n",
        "    def scale_matrix(self):\n",
        "        self.matrix = self.scaler.fit_transform(self.matrix)\n",
        "\n",
        "    def init_tf_idf_vector(self, queries, name_col=None):\n",
        "        if name_col is None:\n",
        "            self.queries_vec = self.tfidf.transform([queries])\n",
        "        else:\n",
        "            self.queries_vec = self.tfidf.transform(queries[name_col])\n",
        "\n",
        "    def scale_vector(self):\n",
        "        self.queries_vec = self.scaler.transform(self.queries_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-DVt84pZ87gp"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "class CoreEngine:\n",
        "    def __init__(self, min_df, max_df, path_docs, path_queries, path_grels, n_neighbors=20, metric='cosine'):\n",
        "        self.dataset = DataSet(path_docs, path_queries, path_grels)\n",
        "        self.dataset.analyez_docs('analyzed_text')\n",
        "        self.tf_idf = TFIDF(min_df, max_df)\n",
        "        self.tf_idf.init_tf_idf_matrix(self.dataset.docs, 'analyzed_text')\n",
        "        self.tf_idf.scale_matrix()\n",
        "        self.nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=metric).fit(self.tf_idf.matrix)\n",
        "\n",
        "    def search(self, query):\n",
        "        query_analyzed = analyze(query)\n",
        "        query_text = \" \".join(query_analyzed)\n",
        "        self.tf_idf.init_tf_idf_vector(query_text)\n",
        "        self.tf_idf.scale_vector()\n",
        "        return self.matching()\n",
        "\n",
        "    def matching(self):\n",
        "        distances, indices = self.nbrs.kneighbors(self.tf_idf.queries_vec)\n",
        "        matching_docs = self.dataset.docs.iloc[indices[0]]\n",
        "        return matching_docs[['doc_id', 'content']].to_dict(orient=\"records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XARPpJF09BCY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, core, k=10):\n",
        "        self.k = k\n",
        "        self.core = core\n",
        "        self.prediction = None\n",
        "\n",
        "    def init_queries_array(self):\n",
        "        self.core.dataset.analyez_queries('analyzed_text')\n",
        "        self.core.tf_idf.init_tf_idf_vector(self.core.dataset.queries, 'analyzed_text')\n",
        "        self.core.tf_idf.scale_vector()\n",
        "\n",
        "    def init_predection_array(self):\n",
        "        distances, indices = self.core.nbrs.kneighbors(self.core.tf_idf.queries_vec)\n",
        "        t1 = pd.melt(pd.DataFrame(distances).reset_index(), id_vars='index').rename(\n",
        "            {'variable': 'col', 'index': 'row', 'value': 'score'}, axis=1)\n",
        "        t2 = pd.melt(pd.DataFrame(indices).reset_index(), id_vars='index').rename(\n",
        "            {'index': 'row', 'variable': 'col', 'value': 'doc_index'}, axis=1)\n",
        "        self.prediction = t1.merge(t2,\n",
        "                                   on=('row', 'col')).merge(self.core.dataset.queries['query_id'].reset_index(),\n",
        "                                                            left_on='row', right_on='index').drop(\n",
        "            ['index', 'row', 'col'],\n",
        "            axis=1).merge(\n",
        "            self.core.dataset.docs['doc_id'].reset_index(),\n",
        "            left_on='doc_index', right_on='index').drop(['index',\n",
        "                                                         'doc_index'], axis=1)\n",
        "        self.prediction['score'] = 1 - self.prediction['score']\n",
        "\n",
        "    def precision_at_k_eva(self, query_id_col='query_id', doc_id_col='doc_id', score_col='score'):\n",
        "        precision_at_k_result = precision_at_k(\n",
        "            self.core.dataset.qrels,\n",
        "            self.prediction,\n",
        "            query_id_col,\n",
        "            doc_id_col,\n",
        "            score_col,\n",
        "            'top_k',\n",
        "            k=self.k)\n",
        "        return precision_at_k_result\n",
        "\n",
        "    def recall_at_k_eva(self, query_id_col='query_id', doc_id_col='doc_id', score_col='score'):\n",
        "        recall_at_k_result = recall_at_k(\n",
        "            self.core.dataset.qrels,\n",
        "            self.prediction,\n",
        "            query_id_col,\n",
        "            doc_id_col,\n",
        "            score_col,\n",
        "            'top_k',\n",
        "            k=self.k)\n",
        "        return recall_at_k_result\n",
        "\n",
        "    def map_at_k_eva(self, query_id_col='query_id', doc_id_col='doc_id', score_col='score'):\n",
        "        map_score_result = map_at_k(\n",
        "            self.core.dataset.qrels,\n",
        "            self.prediction,\n",
        "            query_id_col,\n",
        "            doc_id_col,\n",
        "            score_col,\n",
        "            'top_k',\n",
        "            k=self.k)\n",
        "        return map_score_result\n",
        "\n",
        "    def mean_reciprocal_rank(self, query_id_col='query_id', doc_id_col='doc_id', score_col='score'):\n",
        "        # Merge predictions with ground truth relevance\n",
        "        merged = pd.merge(self.prediction, self.core.dataset.qrels, on=[query_id_col, doc_id_col],\n",
        "                          how='left').fillna(0)\n",
        "        merged = merged.sort_values(by=[query_id_col, score_col], ascending=[True, False])\n",
        "\n",
        "        # Initialize reciprocal ranks\n",
        "        reciprocal_ranks = []\n",
        "\n",
        "        for query_id, group in merged.groupby(query_id_col):\n",
        "            # Get top K results\n",
        "            top_k = group.head(self.k)\n",
        "\n",
        "            # Find the rank of the first relevant document\n",
        "            for rank, (_, row) in enumerate(top_k.iterrows(), start=1):\n",
        "                if row['relevance'] > 0:  # Assuming relevance > 0 indicates relevant document\n",
        "                    reciprocal_ranks.append(1 / rank)\n",
        "                    break\n",
        "            else:\n",
        "                reciprocal_ranks.append(0)\n",
        "\n",
        "        # Calculate MRR\n",
        "        mrr = np.mean(reciprocal_ranks)\n",
        "        return mrr\n",
        "\n",
        "    def measurements(self):\n",
        "        return {\n",
        "            'precision_at_k':self.precision_at_k_eva(),\n",
        "            'recall_at_k':self.recall_at_k_eva(),\n",
        "            'map_at_k':self.map_at_k_eva(),\n",
        "            'mean_reciprocal_rank':self.mean_reciprocal_rank(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-ld5oxCHDRdO"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Recommenders contributors. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "# Default column names\n",
        "DEFAULT_USER_COL = \"userID\"\n",
        "DEFAULT_ITEM_COL = \"itemID\"\n",
        "DEFAULT_RATING_COL = \"rating\"\n",
        "DEFAULT_LABEL_COL = \"label\"\n",
        "DEFAULT_TITLE_COL = \"title\"\n",
        "DEFAULT_GENRE_COL = \"genre\"\n",
        "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
        "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
        "DEFAULT_PREDICTION_COL = \"prediction\"\n",
        "DEFAULT_SIMILARITY_COL = \"sim\"\n",
        "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
        "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\"\n",
        "\n",
        "DEFAULT_HEADER = (\n",
        "    DEFAULT_USER_COL,\n",
        "    DEFAULT_ITEM_COL,\n",
        "    DEFAULT_RATING_COL,\n",
        "    DEFAULT_TIMESTAMP_COL,\n",
        ")\n",
        "\n",
        "COL_DICT = {\n",
        "    \"col_user\": DEFAULT_USER_COL,\n",
        "    \"col_item\": DEFAULT_ITEM_COL,\n",
        "    \"col_rating\": DEFAULT_RATING_COL,\n",
        "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
        "}\n",
        "\n",
        "# Filtering variables\n",
        "DEFAULT_K = 10\n",
        "DEFAULT_THRESHOLD = 10\n",
        "\n",
        "# Other\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YdFSqGUgDWLl"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Recommenders contributors.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import lru_cache, wraps\n",
        "\n",
        "# from recommenders.utils.constants import (\n",
        "#     DEFAULT_USER_COL,\n",
        "#     DEFAULT_ITEM_COL,\n",
        "#     DEFAULT_RATING_COL,\n",
        "#     DEFAULT_LABEL_COL,\n",
        "# )\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def user_item_pairs(\n",
        "    user_df,\n",
        "    item_df,\n",
        "    user_col=DEFAULT_USER_COL,\n",
        "    item_col=DEFAULT_ITEM_COL,\n",
        "    user_item_filter_df=None,\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "):\n",
        "    \"\"\"Get all pairs of users and items data.\n",
        "\n",
        "    Args:\n",
        "        user_df (pandas.DataFrame): User data containing unique user ids and maybe their features.\n",
        "        item_df (pandas.DataFrame): Item data containing unique item ids and maybe their features.\n",
        "        user_col (str): User id column name.\n",
        "        item_col (str): Item id column name.\n",
        "        user_item_filter_df (pd.DataFrame): User-item pairs to be used as a filter.\n",
        "        shuffle (bool): If True, shuffles the result.\n",
        "        seed (int): Random seed for shuffle\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: All pairs of user-item from user_df and item_df, excepting the pairs in user_item_filter_df.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all user-item pairs\n",
        "    user_df[\"key\"] = 1\n",
        "    item_df[\"key\"] = 1\n",
        "    users_items = user_df.merge(item_df, on=\"key\")\n",
        "\n",
        "    user_df.drop(\"key\", axis=1, inplace=True)\n",
        "    item_df.drop(\"key\", axis=1, inplace=True)\n",
        "    users_items.drop(\"key\", axis=1, inplace=True)\n",
        "\n",
        "    # Filter\n",
        "    if user_item_filter_df is not None:\n",
        "        users_items = filter_by(users_items, user_item_filter_df, [user_col, item_col])\n",
        "\n",
        "    if shuffle:\n",
        "        users_items = users_items.sample(frac=1, random_state=seed).reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "\n",
        "    return users_items\n",
        "\n",
        "\n",
        "def filter_by(df, filter_by_df, filter_by_cols):\n",
        "    \"\"\"From the input DataFrame `df`, remove the records whose target column `filter_by_cols` values are\n",
        "    exist in the filter-by DataFrame `filter_by_df`.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Source dataframe.\n",
        "        filter_by_df (pandas.DataFrame): Filter dataframe.\n",
        "        filter_by_cols (iterable of str): Filter columns.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Dataframe filtered by `filter_by_df` on `filter_by_cols`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return df.loc[\n",
        "        ~df.set_index(filter_by_cols).index.isin(\n",
        "            filter_by_df.set_index(filter_by_cols).index\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "class LibffmConverter:\n",
        "    \"\"\"Converts an input dataframe to another dataframe in libffm format. A text file of the converted\n",
        "    Dataframe is optionally generated.\n",
        "\n",
        "    Note:\n",
        "\n",
        "        The input dataframe is expected to represent the feature data in the following schema:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            |field-1|field-2|...|field-n|rating|\n",
        "            |feature-1-1|feature-2-1|...|feature-n-1|1|\n",
        "            |feature-1-2|feature-2-2|...|feature-n-2|0|\n",
        "            ...\n",
        "            |feature-1-i|feature-2-j|...|feature-n-k|0|\n",
        "\n",
        "        Where\n",
        "        1. each `field-*` is the column name of the dataframe (column of label/rating is excluded), and\n",
        "        2. `feature-*-*` can be either a string or a numerical value, representing the categorical variable or\n",
        "        actual numerical variable of the feature value in the field, respectively.\n",
        "        3. If there are ordinal variables represented in int types, users should make sure these columns\n",
        "        are properly converted to string type.\n",
        "\n",
        "        The above data will be converted to the libffm format by following the convention as explained in\n",
        "        `this paper <https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf>`_.\n",
        "\n",
        "        i.e. `<field_index>:<field_feature_index>:1` or `<field_index>:<field_feature_index>:<field_feature_value>`,\n",
        "        depending on the data type of the features in the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): path to save the converted data.\n",
        "\n",
        "    Attributes:\n",
        "        field_count (int): count of field in the libffm format data\n",
        "        feature_count (int): count of feature in the libffm format data\n",
        "        filepath (str or None): file path where the output is stored - it can be None or a string\n",
        "\n",
        "    Examples:\n",
        "        >>> import pandas as pd\n",
        "        >>> df_feature = pd.DataFrame({\n",
        "                'rating': [1, 0, 0, 1, 1],\n",
        "                'field1': ['xxx1', 'xxx2', 'xxx4', 'xxx4', 'xxx4'],\n",
        "                'field2': [3, 4, 5, 6, 7],\n",
        "                'field3': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
        "                'field4': ['1', '2', '3', '4', '5']\n",
        "            })\n",
        "        >>> converter = LibffmConverter().fit(df_feature, col_rating='rating')\n",
        "        >>> df_out = converter.transform(df_feature)\n",
        "        >>> df_out\n",
        "            rating field1 field2   field3 field4\n",
        "        0       1  1:1:1  2:4:3  3:5:1.0  4:6:1\n",
        "        1       0  1:2:1  2:4:4  3:5:2.0  4:7:1\n",
        "        2       0  1:3:1  2:4:5  3:5:3.0  4:8:1\n",
        "        3       1  1:3:1  2:4:6  3:5:4.0  4:9:1\n",
        "        4       1  1:3:1  2:4:7  3:5:5.0  4:10:1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath=None):\n",
        "        self.filepath = filepath\n",
        "        self.col_rating = None\n",
        "        self.field_names = None\n",
        "        self.field_count = None\n",
        "        self.feature_count = None\n",
        "\n",
        "    def fit(self, df, col_rating=DEFAULT_RATING_COL):\n",
        "        \"\"\"Fit the dataframe for libffm format.\n",
        "        This method does nothing but check the validity of the input columns\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "            col_rating (str): rating of the data.\n",
        "\n",
        "        Return:\n",
        "            object: the instance of the converter\n",
        "        \"\"\"\n",
        "\n",
        "        # Check column types.\n",
        "        types = df.dtypes\n",
        "        if not all(\n",
        "            [\n",
        "                x == object or np.issubdtype(x, np.integer) or x == np.float\n",
        "                for x in types\n",
        "            ]\n",
        "        ):\n",
        "            raise TypeError(\"Input columns should be only object and/or numeric types.\")\n",
        "\n",
        "        if col_rating not in df.columns:\n",
        "            raise TypeError(\n",
        "                \"Column of {} is not in input dataframe columns\".format(col_rating)\n",
        "            )\n",
        "\n",
        "        self.col_rating = col_rating\n",
        "        self.field_names = list(df.drop(col_rating, axis=1).columns)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Tranform an input dataset with the same schema (column names and dtypes) to libffm format\n",
        "        by using the fitted converter.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "\n",
        "        Return:\n",
        "            pandas.DataFrame: Output libffm format dataframe.\n",
        "        \"\"\"\n",
        "        if self.col_rating not in df.columns:\n",
        "            raise ValueError(\n",
        "                \"Input dataset does not contain the label column {} in the fitting dataset\".format(\n",
        "                    self.col_rating\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if not all([x in df.columns for x in self.field_names]):\n",
        "            raise ValueError(\n",
        "                \"Not all columns in the input dataset appear in the fitting dataset\"\n",
        "            )\n",
        "\n",
        "        # Encode field-feature.\n",
        "        idx = 1\n",
        "        self.field_feature_dict = {}\n",
        "        for field in self.field_names:\n",
        "            for feature in df[field].values:\n",
        "                # Check whether (field, feature) tuple exists in the dict or not.\n",
        "                # If not, put them into the key-values of the dict and count the index.\n",
        "                if (field, feature) not in self.field_feature_dict:\n",
        "                    self.field_feature_dict[(field, feature)] = idx\n",
        "                    if df[field].dtype == object:\n",
        "                        idx += 1\n",
        "            if df[field].dtype != object:\n",
        "                idx += 1\n",
        "\n",
        "        self.field_count = len(self.field_names)\n",
        "        self.feature_count = idx - 1\n",
        "\n",
        "        def _convert(field, feature, field_index, field_feature_index_dict):\n",
        "            field_feature_index = field_feature_index_dict[(field, feature)]\n",
        "            if isinstance(feature, str):\n",
        "                feature = 1\n",
        "            return \"{}:{}:{}\".format(field_index, field_feature_index, feature)\n",
        "\n",
        "        for col_index, col in enumerate(self.field_names):\n",
        "            df[col] = df[col].apply(\n",
        "                lambda x: _convert(col, x, col_index + 1, self.field_feature_dict)\n",
        "            )\n",
        "\n",
        "        # Move rating column to the first.\n",
        "        column_names = self.field_names[:]\n",
        "        column_names.insert(0, self.col_rating)\n",
        "        df = df[column_names]\n",
        "\n",
        "        if self.filepath is not None:\n",
        "            np.savetxt(self.filepath, df.values, delimiter=\" \", fmt=\"%s\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, df, col_rating=DEFAULT_RATING_COL):\n",
        "        \"\"\"Do fit and transform in a row\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "            col_rating (str): rating of the data.\n",
        "\n",
        "        Return:\n",
        "            pandas.DataFrame: Output libffm format dataframe.\n",
        "        \"\"\"\n",
        "        return self.fit(df, col_rating=col_rating).transform(df)\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"Get parameters (attributes) of the libffm converter\n",
        "\n",
        "        Return:\n",
        "            dict: A dictionary that contains parameters field count, feature count, and file path.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"field count\": self.field_count,\n",
        "            \"feature count\": self.feature_count,\n",
        "            \"file path\": self.filepath,\n",
        "        }\n",
        "\n",
        "\n",
        "def negative_feedback_sampler(\n",
        "    df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_label=DEFAULT_LABEL_COL,\n",
        "    col_feedback=\"feedback\",\n",
        "    ratio_neg_per_user=1,\n",
        "    pos_value=1,\n",
        "    neg_value=0,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Utility function to sample negative feedback from user-item interaction dataset.\n",
        "    This negative sampling function will take the user-item interaction data to create\n",
        "    binarized feedback, i.e., 1 and 0 indicate positive and negative feedback,\n",
        "    respectively.\n",
        "\n",
        "    Negative sampling is used in the literature frequently to generate negative samples\n",
        "    from a user-item interaction data.\n",
        "\n",
        "    See for example the `neural collaborative filtering paper <https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): input data that contains user-item tuples.\n",
        "        col_user (str): user id column name.\n",
        "        col_item (str): item id column name.\n",
        "        col_label (str): label column name in df.\n",
        "        col_feedback (str): feedback column name in the returned data frame; it is used for the generated column\n",
        "            of positive and negative feedback.\n",
        "        ratio_neg_per_user (int): ratio of negative feedback w.r.t to the number of positive feedback for each user.\n",
        "            If the samples exceed the number of total possible negative feedback samples, it will be reduced to the\n",
        "            number of all the possible samples.\n",
        "        pos_value (float): value of positive feedback.\n",
        "        neg_value (float): value of negative feedback.\n",
        "        inplace (bool):\n",
        "        seed (int): seed for the random state of the sampling function.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Data with negative feedback.\n",
        "\n",
        "    Examples:\n",
        "        >>> import pandas as pd\n",
        "        >>> df = pd.DataFrame({\n",
        "            'userID': [1, 2, 3],\n",
        "            'itemID': [1, 2, 3],\n",
        "            'rating': [5, 5, 5]\n",
        "        })\n",
        "        >>> df_neg_sampled = negative_feedback_sampler(\n",
        "            df, col_user='userID', col_item='itemID', ratio_neg_per_user=1\n",
        "        )\n",
        "        >>> df_neg_sampled\n",
        "        userID  itemID  feedback\n",
        "        1   1   1\n",
        "        1   2   0\n",
        "        2   2   1\n",
        "        2   1   0\n",
        "        3   3   1\n",
        "        3   1   0\n",
        "    \"\"\"\n",
        "    # Get all of the users and items.\n",
        "    items = df[col_item].unique()\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "    def sample_items(user_df):\n",
        "        # Sample negative items for the data frame restricted to a specific user\n",
        "        n_u = len(user_df)\n",
        "        neg_sample_size = max(round(n_u * ratio_neg_per_user), 1)\n",
        "        # Draw (n_u + neg_sample_size) items and keep neg_sample_size of these\n",
        "        # that are not already in user_df. This requires a set difference from items_sample\n",
        "        # instead of items, which is more efficient when len(items) is large.\n",
        "        sample_size = min(n_u + neg_sample_size, len(items))\n",
        "        items_sample = rng.choice(items, sample_size, replace=False)\n",
        "        new_items = np.setdiff1d(items_sample, user_df[col_item])[:neg_sample_size]\n",
        "        new_df = pd.DataFrame(\n",
        "            data={\n",
        "                col_user: user_df.name,\n",
        "                col_item: new_items,\n",
        "                col_label: neg_value,\n",
        "            }\n",
        "        )\n",
        "        return pd.concat([user_df, new_df], ignore_index=True)\n",
        "\n",
        "    res_df = df.copy()\n",
        "    res_df[col_label] = pos_value\n",
        "    return (\n",
        "        res_df.groupby(col_user)\n",
        "        .apply(sample_items)\n",
        "        .reset_index(drop=True)\n",
        "        .rename(columns={col_label: col_feedback})\n",
        "    )\n",
        "\n",
        "\n",
        "def has_columns(df, columns):\n",
        "    \"\"\"Check if DataFrame has necessary columns\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame\n",
        "        columns (iterable(str)): columns to check for\n",
        "\n",
        "    Returns:\n",
        "        bool: True if DataFrame has specified columns.\n",
        "    \"\"\"\n",
        "    if not isinstance(columns, set):\n",
        "        columns = set(columns)\n",
        "    return columns.issubset(df.columns)\n",
        "\n",
        "\n",
        "def has_same_base_dtype(df_1, df_2, columns=None):\n",
        "    \"\"\"Check if specified columns have the same base dtypes across both DataFrames\n",
        "\n",
        "    Args:\n",
        "        df_1 (pandas.DataFrame): first DataFrame\n",
        "        df_2 (pandas.DataFrame): second DataFrame\n",
        "        columns (list(str)): columns to check, None checks all columns\n",
        "\n",
        "    Returns:\n",
        "        bool: True if DataFrames columns have the same base dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    if columns is None:\n",
        "        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):\n",
        "            logger.error(\n",
        "                \"Cannot test all columns because they are not all shared across DataFrames\"\n",
        "            )\n",
        "            return False\n",
        "        columns = df_1.columns\n",
        "\n",
        "    if not (\n",
        "        has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    result = True\n",
        "    for column in columns:\n",
        "        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:\n",
        "            logger.error(\"Columns {} do not have the same base datatype\".format(column))\n",
        "            result = False\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "class PandasHash:\n",
        "    \"\"\"Wrapper class to allow pandas objects (DataFrames or Series) to be hashable\"\"\"\n",
        "\n",
        "    # reserve space just for a single pandas object\n",
        "    __slots__ = \"pandas_object\"\n",
        "\n",
        "    def __init__(self, pandas_object):\n",
        "        \"\"\"Initialize class\n",
        "\n",
        "        Args:\n",
        "            pandas_object (pandas.DataFrame|pandas.Series): pandas object\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(pandas_object, (pd.DataFrame, pd.Series)):\n",
        "            raise TypeError(\"Can only wrap pandas DataFrame or Series objects\")\n",
        "        self.pandas_object = pandas_object\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"Overwrite equality comparison\n",
        "\n",
        "        Args:\n",
        "            other (pandas.DataFrame|pandas.Series): pandas object to compare\n",
        "\n",
        "        Returns:\n",
        "            bool: whether other object is the same as this one\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(self) == hash(other)\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"Overwrite hash operator for use with pandas objects\n",
        "\n",
        "        Returns:\n",
        "            int: hashed value of object\n",
        "        \"\"\"\n",
        "\n",
        "        hashable = tuple(self.pandas_object.values.tobytes())\n",
        "        if isinstance(self.pandas_object, pd.DataFrame):\n",
        "            hashable += tuple(self.pandas_object.columns)\n",
        "        else:\n",
        "            hashable += tuple(self.pandas_object.name)\n",
        "        return hash(hashable)\n",
        "\n",
        "\n",
        "def lru_cache_df(maxsize, typed=False):\n",
        "    \"\"\"Least-recently-used cache decorator for pandas Dataframes.\n",
        "\n",
        "    Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can\n",
        "    save time when an expensive or I/O bound function is periodically called with the same arguments.\n",
        "\n",
        "    Inspired in the `lru_cache function <https://docs.python.org/3/library/functools.html#functools.lru_cache>`_.\n",
        "\n",
        "    Args:\n",
        "        maxsize (int|None): max size of cache, if set to None cache is boundless\n",
        "        typed (bool): arguments of different types are cached separately\n",
        "    \"\"\"\n",
        "\n",
        "    def to_pandas_hash(val):\n",
        "        \"\"\"Return PandaHash object if input is a DataFrame otherwise return input unchanged\"\"\"\n",
        "        return PandasHash(val) if isinstance(val, pd.DataFrame) else val\n",
        "\n",
        "    def from_pandas_hash(val):\n",
        "        \"\"\"Extract DataFrame if input is PandaHash object otherwise return input unchanged\"\"\"\n",
        "        return val.pandas_object if isinstance(val, PandasHash) else val\n",
        "\n",
        "    def decorating_function(user_function):\n",
        "        @wraps(user_function)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            # convert DataFrames in args and kwargs to PandaHash objects\n",
        "            args = tuple([to_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: to_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return cached_wrapper(*args, **kwargs)\n",
        "\n",
        "        @lru_cache(maxsize=maxsize, typed=typed)\n",
        "        def cached_wrapper(*args, **kwargs):\n",
        "            # get DataFrames from PandaHash objects in args and kwargs\n",
        "            args = tuple([from_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: from_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return user_function(*args, **kwargs)\n",
        "\n",
        "        # retain lru_cache attributes\n",
        "        wrapper.cache_info = cached_wrapper.cache_info\n",
        "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorating_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KY1r743UFAs6"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Recommenders contributors.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import wraps\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    explained_variance_score,\n",
        "    roc_auc_score,\n",
        "    log_loss,\n",
        ")\n",
        "\n",
        "# from recommenders.utils.constants import (\n",
        "#     DEFAULT_USER_COL,\n",
        "#     DEFAULT_ITEM_COL,\n",
        "#     DEFAULT_RATING_COL,\n",
        "#     DEFAULT_PREDICTION_COL,\n",
        "#     DEFAULT_RELEVANCE_COL,\n",
        "#     DEFAULT_SIMILARITY_COL,\n",
        "#     DEFAULT_ITEM_FEATURES_COL,\n",
        "#     DEFAULT_ITEM_SIM_MEASURE,\n",
        "#     DEFAULT_K,\n",
        "#     DEFAULT_THRESHOLD,\n",
        "# )\n",
        "# from recommenders.datasets.pandas_df_utils import (\n",
        "#     has_columns,\n",
        "#     has_same_base_dtype,\n",
        "#     lru_cache_df,\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ColumnMismatchError(Exception):\n",
        "    \"\"\"Exception raised when there is a mismatch in columns.\n",
        "\n",
        "    This exception is raised when an operation involving columns\n",
        "    encounters a mismatch or inconsistency.\n",
        "\n",
        "    Attributes:\n",
        "        message (str): Explanation of the error.\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ColumnTypeMismatchError(Exception):\n",
        "    \"\"\"Exception raised when there is a mismatch in column types.\n",
        "\n",
        "    This exception is raised when an operation involving column types\n",
        "    encounters a mismatch or inconsistency.\n",
        "\n",
        "    Attributes:\n",
        "        message (str): Explanation of the error.\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "def _check_column_dtypes(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "\n",
        "    This includes the checks on:\n",
        "\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_wrapper(\n",
        "        rating_true,\n",
        "        rating_pred,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "\n",
        "        Args:\n",
        "            rating_true (pandas.DataFrame): True data\n",
        "            rating_pred (pandas.DataFrame): Predicted data\n",
        "            col_user (str): column name for user\n",
        "            col_item (str): column name for item\n",
        "            col_rating (str): column name for rating\n",
        "            col_prediction (str): column name for prediction\n",
        "        \"\"\"\n",
        "        # Some ranking metrics don't have the rating column, so we don't need to check.\n",
        "        expected_true_columns = {col_user, col_item}\n",
        "        if \"col_rating\" in kwargs:\n",
        "            expected_true_columns.add(kwargs[\"col_rating\"])\n",
        "        if not has_columns(rating_true, expected_true_columns):\n",
        "            raise ColumnMismatchError(\"Missing columns in true rating DataFrame\")\n",
        "\n",
        "        if not has_columns(rating_pred, {col_user, col_item, col_prediction}):\n",
        "            raise ColumnMismatchError(\"Missing columns in predicted rating DataFrame\")\n",
        "\n",
        "        if not has_same_base_dtype(\n",
        "            rating_true, rating_pred, columns=[col_user, col_item]\n",
        "        ):\n",
        "            raise ColumnTypeMismatchError(\n",
        "                \"Columns in provided DataFrames are not the same datatype\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            rating_true=rating_true,\n",
        "            rating_pred=rating_pred,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            col_prediction=col_prediction,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_wrapper\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes\n",
        "@lru_cache_df(maxsize=1)\n",
        "def merge_rating_true_pred(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Join truth and prediction data frames on userID and itemID and return the true\n",
        "    and predicted rated with the correct index.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array with the true ratings\n",
        "        numpy.ndarray: Array with the predicted ratings\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # pd.merge will apply suffixes to columns which have the same name across both dataframes\n",
        "    suffixes = [\"_true\", \"_pred\"]\n",
        "    rating_true_pred = pd.merge(\n",
        "        rating_true, rating_pred, on=[col_user, col_item], suffixes=suffixes\n",
        "    )\n",
        "    if col_rating in rating_pred.columns:\n",
        "        col_rating = col_rating + suffixes[0]\n",
        "    if col_prediction in rating_true.columns:\n",
        "        col_prediction = col_prediction + suffixes[1]\n",
        "    return rating_true_pred[col_rating], rating_true_pred[col_prediction]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rmse(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate Root Mean Squared Error\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: Root mean squared error\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mae(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate Mean Absolute Error.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: Mean Absolute Error.\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rsquared(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate R squared\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: R squared (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return r2_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exp_var(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate explained variance.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: Explained variance (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return explained_variance_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def auc(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate the Area-Under-Curve metric for implicit feedback typed\n",
        "    recommender, where rating is binary and prediction is float number ranging\n",
        "    from 0 to 1.\n",
        "\n",
        "    https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
        "\n",
        "    Note:\n",
        "        The evaluation does not require a leave-one-out scenario.\n",
        "        This metric does not calculate group-based AUC which considers the AUC scores\n",
        "        averaged across users. It is also not limited to k. Instead, it calculates the\n",
        "        scores on the entire prediction results regardless the users.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: auc_score (min=0, max=1)\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return roc_auc_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def logloss(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate the logloss metric for implicit feedback typed\n",
        "    recommender, where rating is binary and prediction is float number ranging\n",
        "    from 0 to 1.\n",
        "\n",
        "    https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss)\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "\n",
        "    Returns:\n",
        "        float: log_loss_score (min=-inf, max=inf)\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return log_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes\n",
        "@lru_cache_df(maxsize=1)\n",
        "def merge_ranking_true_pred(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user,\n",
        "    col_item,\n",
        "    col_prediction,\n",
        "    relevancy_method,\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Filter truth and prediction data frames on common users\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user (optional)\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame, pandas.DataFrame, int: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
        "        DataFrame of hit counts vs actual relevant items per user number of unique user ids\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the prediction and true data frames have the same set of users\n",
        "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
        "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
        "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
        "    n_users = len(common_users)\n",
        "\n",
        "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
        "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
        "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
        "    # to calculate penalized precision of the ordered items.\n",
        "    if relevancy_method == \"top_k\":\n",
        "        top_k = k\n",
        "    elif relevancy_method == \"by_threshold\":\n",
        "        top_k = threshold\n",
        "    elif relevancy_method is None:\n",
        "        top_k = None\n",
        "    else:\n",
        "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
        "    df_hit = get_top_k_items(\n",
        "        dataframe=rating_pred_common,\n",
        "        col_user=col_user,\n",
        "        col_rating=col_prediction,\n",
        "        k=top_k,\n",
        "    )\n",
        "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
        "        [col_user, col_item, \"rank\"]\n",
        "    ]\n",
        "\n",
        "    # count the number of hits vs actual relevant items per user\n",
        "    df_hit_count = pd.merge(\n",
        "        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
        "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
        "            {\"actual\": \"count\"}\n",
        "        ),\n",
        "        on=col_user,\n",
        "    )\n",
        "\n",
        "    return df_hit, df_hit_count, n_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def precision_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Precision at K.\n",
        "\n",
        "    Note:\n",
        "        We use the same formula to calculate precision@k as that in Spark.\n",
        "        More details can be found at\n",
        "        http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt\n",
        "        In particular, the maximum achievable precision may be < 1, if the number of items for a\n",
        "        user in rating_pred is less than k.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: precision at k (min=0, max=1)\n",
        "    \"\"\"\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (df_hit_count[\"hit\"] / k).sum() / n_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def recall_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Recall at K.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n",
        "        k items exist for a user in rating_true.\n",
        "    \"\"\"\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def r_precision_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"R-precision at K.\n",
        "\n",
        "    R-precision can be defined as the precision@R for each user, where R is the\n",
        "    numer of relevant items for the query. Its also equivalent to the recall at\n",
        "    the R-th position.\n",
        "\n",
        "    Note:\n",
        "        As R can be high, in this case, the k indicates the maximum possible R.\n",
        "        If every user has more than k true items, then r-precision@k is equal to\n",
        "        precision@k. You might need to raise the k value to get meaningful results.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n",
        "        k items exist for a user in rating_true.\n",
        "    \"\"\"\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    df_merged = df_hit.merge(df_hit_count[[col_user, 'actual']])\n",
        "    df_merged = df_merged[df_merged['rank'] <= df_merged['actual']]\n",
        "\n",
        "    return (df_merged.groupby(col_user).size() / df_hit_count.set_index(col_user)['actual']).mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ndcg_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    score_type=\"binary\",\n",
        "    discfun_type=\"loge\",\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Normalized Discounted Cumulative Gain (nDCG).\n",
        "\n",
        "    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "        score_type (str): type of relevance scores ['binary', 'raw', 'exp']. With the default option 'binary', the\n",
        "            relevance score is reduced to either 1 (hit) or 0 (miss). Option 'raw' uses the raw relevance score.\n",
        "            Option 'exp' uses (2 ** RAW_RELEVANCE - 1) as the relevance score\n",
        "        discfun_type (str): type of discount function ['loge', 'log2'] used to calculate DCG.\n",
        "\n",
        "    Returns:\n",
        "        float: nDCG at k (min=0, max=1).\n",
        "    \"\"\"\n",
        "    df_hit, _, _ = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    df_dcg = df_hit.merge(rating_pred, on=[col_user, col_item]).merge(\n",
        "        rating_true, on=[col_user, col_item], how=\"outer\", suffixes=(\"_left\", None)\n",
        "    )\n",
        "\n",
        "    if score_type == \"binary\":\n",
        "        df_dcg[\"rel\"] = 1\n",
        "    elif score_type == \"raw\":\n",
        "        df_dcg[\"rel\"] = df_dcg[col_rating]\n",
        "    elif score_type == \"exp\":\n",
        "        df_dcg[\"rel\"] = 2 ** df_dcg[col_rating] - 1\n",
        "    else:\n",
        "        raise ValueError(\"score_type must be one of 'binary', 'raw', 'exp'\")\n",
        "\n",
        "    if discfun_type == \"loge\":\n",
        "        discfun = np.log\n",
        "    elif discfun_type == \"log2\":\n",
        "        discfun = np.log2\n",
        "    else:\n",
        "        raise ValueError(\"discfun_type must be one of 'loge', 'log2'\")\n",
        "\n",
        "    # Calculate the actual discounted gain for each record\n",
        "    df_dcg[\"dcg\"] = df_dcg[\"rel\"] / discfun(1 + df_dcg[\"rank\"])\n",
        "\n",
        "    # Calculate the ideal discounted gain for each record\n",
        "    df_idcg = df_dcg.sort_values([col_user, col_rating], ascending=False)\n",
        "    df_idcg[\"irank\"] = df_idcg.groupby(col_user, as_index=False, sort=False)[\n",
        "        col_rating\n",
        "    ].rank(\"first\", ascending=False)\n",
        "    df_idcg[\"idcg\"] = df_idcg[\"rel\"] / discfun(1 + df_idcg[\"irank\"])\n",
        "\n",
        "    # Calculate the actual DCG for each user\n",
        "    df_user = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
        "\n",
        "    # Calculate the ideal DCG for each user\n",
        "    df_user = df_user.merge(\n",
        "        df_idcg.groupby(col_user, as_index=False, sort=False)\n",
        "        .head(k)\n",
        "        .groupby(col_user, as_index=False, sort=False)\n",
        "        .agg({\"idcg\": \"sum\"}),\n",
        "        on=col_user,\n",
        "    )\n",
        "\n",
        "    # DCG over IDCG is the normalized DCG\n",
        "    df_user[\"ndcg\"] = df_user[\"dcg\"] / df_user[\"idcg\"]\n",
        "    return df_user[\"ndcg\"].mean()\n",
        "\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_reciprocal_rank(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return None, n_users\n",
        "\n",
        "    # calculate reciprocal rank of items for each user and sum them up\n",
        "    df_hit_sorted = df_hit.copy()\n",
        "    df_hit_sorted[\"rr\"] = (\n",
        "        df_hit_sorted.groupby(col_user).cumcount() + 1\n",
        "    ) / df_hit_sorted[\"rank\"]\n",
        "    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n",
        "\n",
        "    return pd.merge(df_hit_sorted, df_hit_count, on=col_user), n_users\n",
        "\n",
        "\n",
        "\n",
        "def map(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Mean Average Precision for top k prediction items\n",
        "\n",
        "    The implementation of MAP is referenced from Spark MLlib evaluation metrics.\n",
        "    https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems\n",
        "\n",
        "    A good reference can be found at:\n",
        "    http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
        "\n",
        "    Note:\n",
        "        The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of\n",
        "        relevant items in the ground truth data, instead of k.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: MAP (min=0, max=1)\n",
        "    \"\"\"\n",
        "    df_merge, n_users = _get_reciprocal_rank(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_merge is None:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return (df_merge[\"rr\"] / df_merge[\"actual\"]).sum() / n_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def map_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Mean Average Precision at k\n",
        "\n",
        "    The implementation of MAP@k is referenced from Spark MLlib evaluation metrics.\n",
        "    https://github.com/apache/spark/blob/b938ff9f520fd4e4997938284ffa0aba9ea271fc/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L99\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: MAP@k (min=0, max=1)\n",
        "    \"\"\"\n",
        "    df_merge, n_users = _get_reciprocal_rank(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_merge is None:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return (\n",
        "            df_merge[\"rr\"] / df_merge[\"actual\"].apply(lambda x: min(x, k))\n",
        "        ).sum() / n_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_items(\n",
        "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
        "):\n",
        "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
        "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
        "    for each user.\n",
        "\n",
        "    Note:\n",
        "        If it is implicit rating, just append a column of constants to be\n",
        "        ratings.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
        "        customerID-itemID-rating)\n",
        "        col_user (str): column name for user\n",
        "        col_rating (str): column name for rating\n",
        "        k (int or None): number of items for each user; None means that the input has already been\n",
        "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
        "    \"\"\"\n",
        "    # Sort dataframe by col_user and (top k) col_rating\n",
        "    if k is None:\n",
        "        top_k_items = dataframe\n",
        "    else:\n",
        "        top_k_items = (\n",
        "            dataframe.sort_values([col_user, col_rating], ascending=[True, False])\n",
        "            .groupby(col_user, as_index=False)\n",
        "            .head(k)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "    # Add ranks\n",
        "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
        "    return top_k_items\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Function name and function mapper.\n",
        "Useful when we have to serialize evaluation metric names\n",
        "and call the functions based on deserialized names\"\"\"\n",
        "metrics = {\n",
        "    rmse.__name__: rmse,\n",
        "    mae.__name__: mae,\n",
        "    rsquared.__name__: rsquared,\n",
        "    exp_var.__name__: exp_var,\n",
        "    precision_at_k.__name__: precision_at_k,\n",
        "    recall_at_k.__name__: recall_at_k,\n",
        "    r_precision_at_k.__name__: r_precision_at_k,\n",
        "    ndcg_at_k.__name__: ndcg_at_k,\n",
        "    map_at_k.__name__: map_at_k,\n",
        "    map.__name__: map,\n",
        "}\n",
        "\n",
        "\n",
        "# diversity metrics\n",
        "def _check_column_dtypes_diversity_serendipity(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "\n",
        "    This includes the checks on:\n",
        "\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "    * whether reco_df contains any user_item pairs that are already shown in train_df\n",
        "    * check relevance column in reco_df\n",
        "    * check column names in item_feature_df\n",
        "\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_diversity_serendipity_wrapper(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df=None,\n",
        "        item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "        col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_sim=DEFAULT_SIMILARITY_COL,\n",
        "        col_relevance=None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "\n",
        "        Args:\n",
        "            train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "            reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "            item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "                It contains two columns: col_item and features (a feature vector).\n",
        "            item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "                Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "            col_item_features (str): item feature column name.\n",
        "            col_user (str): User id column name.\n",
        "            col_item (str): Item id column name.\n",
        "            col_sim (str): This column indicates the column name for item similarity.\n",
        "            col_relevance (str): This column indicates whether the recommended item is actually\n",
        "                relevant to the user or not.\n",
        "        \"\"\"\n",
        "\n",
        "        if not has_columns(train_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
        "        if not has_columns(reco_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
        "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
        "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
        "        if col_relevance is None:\n",
        "            col_relevance = DEFAULT_RELEVANCE_COL\n",
        "            # relevance term, default is 1 (relevant) for all\n",
        "            reco_df = reco_df[[col_user, col_item]]\n",
        "            reco_df[col_relevance] = 1.0\n",
        "        else:\n",
        "            col_relevance = col_relevance\n",
        "            reco_df = reco_df[[col_user, col_item, col_relevance]].astype(\n",
        "                {col_relevance: np.float16}\n",
        "            )\n",
        "        if item_sim_measure == \"item_feature_vector\":\n",
        "            required_columns = [col_item, col_item_features]\n",
        "            if item_feature_df is not None:\n",
        "                if not has_columns(item_feature_df, required_columns):\n",
        "                    raise ValueError(\"Missing columns in item_feature_df DataFrame\")\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"item_feature_df not specified! item_feature_df must be provided \"\n",
        "                    \"if choosing to use item_feature_vector to calculate item similarity. \"\n",
        "                    \"item_feature_df should have columns: \" + str(required_columns)\n",
        "                )\n",
        "        # check if reco_df contains any user_item pairs that are already shown in train_df\n",
        "        count_intersection = pd.merge(\n",
        "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
        "        ).shape[0]\n",
        "        if count_intersection != 0:\n",
        "            raise Exception(\n",
        "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            train_df=train_df,\n",
        "            reco_df=reco_df,\n",
        "            item_feature_df=item_feature_df,\n",
        "            item_sim_measure=item_sim_measure,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            col_sim=col_sim,\n",
        "            col_relevance=col_relevance,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_diversity_serendipity_wrapper\n",
        "\n",
        "\n",
        "def _check_column_dtypes_novelty_coverage(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "\n",
        "    This includes the checks on:\n",
        "\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "    * whether reco_df contains any user_item pairs that are already shown in train_df\n",
        "\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_novelty_coverage_wrapper(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "\n",
        "        Args:\n",
        "            train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "            reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "            col_user (str): User id column name.\n",
        "            col_item (str): Item id column name.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if not has_columns(train_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
        "        if not has_columns(reco_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
        "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
        "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
        "\n",
        "        count_intersection = pd.merge(\n",
        "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
        "        ).shape[0]\n",
        "        if count_intersection != 0:\n",
        "            raise Exception(\n",
        "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            train_df=train_df,\n",
        "            reco_df=reco_df,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_novelty_coverage_wrapper\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_pairwise_items(\n",
        "    df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Get pairwise combinations of items per user (ignoring duplicate pairs [1,2] == [2,1])\"\"\"\n",
        "    df_user_i1 = df[[col_user, col_item]]\n",
        "    df_user_i1.columns = [col_user, \"i1\"]\n",
        "\n",
        "    df_user_i2 = df[[col_user, col_item]]\n",
        "    df_user_i2.columns = [col_user, \"i2\"]\n",
        "\n",
        "    df_user_i1_i2 = pd.merge(df_user_i1, df_user_i2, how=\"inner\", on=[col_user])\n",
        "\n",
        "    df_pairwise_items = df_user_i1_i2[(df_user_i1_i2[\"i1\"] <= df_user_i1_i2[\"i2\"])][\n",
        "        [col_user, \"i1\", \"i2\"]\n",
        "    ].reset_index(drop=True)\n",
        "    return df_pairwise_items\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_cosine_similarity(\n",
        "    train_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    if item_sim_measure == \"item_cooccurrence_count\":\n",
        "        # calculate item-item similarity based on item co-occurrence count\n",
        "        df_cosine_similarity = _get_cooccurrence_similarity(\n",
        "            train_df, col_user, col_item, col_sim\n",
        "        )\n",
        "    elif item_sim_measure == \"item_feature_vector\":\n",
        "        # calculdf_cosine_similarity = ate item-item similarity based on item feature vectors\n",
        "        df_cosine_similarity = _get_item_feature_similarity(\n",
        "            item_feature_df, col_item_features, col_user, col_item\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(\n",
        "            \"item_sim_measure not recognized! The available options include 'item_cooccurrence_count' and 'item_feature_vector'.\"\n",
        "        )\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_cooccurrence_similarity(\n",
        "    train_df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Cosine similarity metric from\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:\n",
        "        introducing serendipity into music recommendation, WSDM 2012\n",
        "\n",
        "    The item indexes in the result are such that i1 <= i2.\n",
        "    \"\"\"\n",
        "    pairs = _get_pairwise_items(train_df, col_user, col_item)\n",
        "    pairs_count = pd.DataFrame(\n",
        "        {\"count\": pairs.groupby([\"i1\", \"i2\"]).size()}\n",
        "    ).reset_index()\n",
        "    item_count = pd.DataFrame(\n",
        "        {\"count\": train_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    item_count[\"item_sqrt_count\"] = item_count[\"count\"] ** 0.5\n",
        "    item_co_occur = pairs_count.merge(\n",
        "        item_count[[col_item, \"item_sqrt_count\"]],\n",
        "        left_on=[\"i1\"],\n",
        "        right_on=[col_item],\n",
        "    ).drop(columns=[col_item])\n",
        "\n",
        "    item_co_occur.columns = [\"i1\", \"i2\", \"count\", \"i1_sqrt_count\"]\n",
        "\n",
        "    item_co_occur = item_co_occur.merge(\n",
        "        item_count[[col_item, \"item_sqrt_count\"]],\n",
        "        left_on=[\"i2\"],\n",
        "        right_on=[col_item],\n",
        "    ).drop(columns=[col_item])\n",
        "    item_co_occur.columns = [\n",
        "        \"i1\",\n",
        "        \"i2\",\n",
        "        \"count\",\n",
        "        \"i1_sqrt_count\",\n",
        "        \"i2_sqrt_count\",\n",
        "    ]\n",
        "\n",
        "    item_co_occur[col_sim] = item_co_occur[\"count\"] / (\n",
        "        item_co_occur[\"i1_sqrt_count\"] * item_co_occur[\"i2_sqrt_count\"]\n",
        "    )\n",
        "    df_cosine_similarity = (\n",
        "        item_co_occur[[\"i1\", \"i2\", col_sim]]\n",
        "        .sort_values([\"i1\", \"i2\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_item_feature_similarity(\n",
        "    item_feature_df,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Cosine similarity metric based on item feature vectors\n",
        "\n",
        "    The item indexes in the result are such that i1 <= i2.\n",
        "    \"\"\"\n",
        "    df1 = item_feature_df[[col_item, col_item_features]]\n",
        "    df1.columns = [\"i1\", \"f1\"]\n",
        "    df1[\"key\"] = 0\n",
        "    df2 = item_feature_df[[col_item, col_item_features]]\n",
        "    df2.columns = [\"i2\", \"f2\"]\n",
        "    df2[\"key\"] = 0\n",
        "\n",
        "    df = pd.merge(df1, df2, on=\"key\", how=\"outer\").drop(\"key\", axis=1)\n",
        "    df_item_feature_pair = df[(df[\"i1\"] <= df[\"i2\"])].reset_index(drop=True)\n",
        "\n",
        "    df_item_feature_pair[col_sim] = df_item_feature_pair.apply(\n",
        "        lambda x: float(x.f1.dot(x.f2))\n",
        "        / float(np.linalg.norm(x.f1, 2) * np.linalg.norm(x.f2, 2)),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    df_cosine_similarity = df_item_feature_pair[[\"i1\", \"i2\", col_sim]].sort_values(\n",
        "        [\"i1\", \"i2\"]\n",
        "    )\n",
        "\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "# Diversity metrics\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_intralist_similarity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Intra-list similarity from\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        \"Improving Recommendation Lists Through Topic Diversification\",\n",
        "        Ziegler, McNee, Konstan and Lausen, 2005.\n",
        "    \"\"\"\n",
        "    pairs = _get_pairwise_items(reco_df, col_user, col_item)\n",
        "    similarity_df = _get_cosine_similarity(\n",
        "        train_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    # Fillna(0) is needed in the cases where similarity_df does not have an entry for a pair of items.\n",
        "    # e.g. i1 and i2 have never occurred together.\n",
        "\n",
        "    item_pair_sim = pairs.merge(similarity_df, on=[\"i1\", \"i2\"], how=\"left\")\n",
        "    item_pair_sim[col_sim].fillna(0, inplace=True)\n",
        "    item_pair_sim = item_pair_sim.loc[\n",
        "        item_pair_sim[\"i1\"] != item_pair_sim[\"i2\"]\n",
        "    ].reset_index(drop=True)\n",
        "    df_intralist_similarity = (\n",
        "        item_pair_sim.groupby([col_user]).agg({col_sim: \"mean\"}).reset_index()\n",
        "    )\n",
        "    df_intralist_similarity.columns = [col_user, \"avg_il_sim\"]\n",
        "\n",
        "    return df_intralist_similarity\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "@lru_cache_df(maxsize=1)\n",
        "def user_diversity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average diversity of recommendations for each user.\n",
        "    The metric definition is based on formula (3) in the following reference:\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:\n",
        "        introducing serendipity into music recommendation, WSDM 2012\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they have interacted with;\n",
        "            contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item, col_relevance (optional).\n",
        "            Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually relevant to the user or not.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with the following columns: col_user, user_diversity.\n",
        "    \"\"\"\n",
        "\n",
        "    df_intralist_similarity = _get_intralist_similarity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    df_user_diversity = df_intralist_similarity\n",
        "    df_user_diversity[\"user_diversity\"] = 1 - df_user_diversity[\"avg_il_sim\"]\n",
        "    df_user_diversity = (\n",
        "        df_user_diversity[[col_user, \"user_diversity\"]]\n",
        "        .sort_values(col_user)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_user_diversity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def diversity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average diversity of recommendations across all users.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they have interacted with;\n",
        "            contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item, col_relevance (optional).\n",
        "            Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually relevant to the user or not.\n",
        "\n",
        "    Returns:\n",
        "        float: diversity.\n",
        "    \"\"\"\n",
        "    df_user_diversity = user_diversity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    avg_diversity = df_user_diversity.agg({\"user_diversity\": \"mean\"})[0]\n",
        "    return avg_diversity\n",
        "\n",
        "\n",
        "\n",
        "# Novelty metrics\n",
        "\n",
        "\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "@lru_cache_df(maxsize=1)\n",
        "def historical_item_novelty(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Calculate novelty for each item. Novelty is computed as the minus logarithm of\n",
        "    (number of interactions with item / total number of interactions). The definition of the metric\n",
        "    is based on the following reference using the choice model (eqs. 1 and 6):\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:\n",
        "        choice, discovery and relevance, ECIR 2011\n",
        "\n",
        "    The novelty of an item can be defined relative to a set of observed events on the set of all items.\n",
        "    These can be events of user choice (item \"is picked\" by a random user) or user discovery\n",
        "    (item \"is known\" to a random user). The above definition of novelty reflects a factor of item popularity.\n",
        "    High novelty values correspond to long-tail items in the density function, that few users have interacted\n",
        "    with and low novelty values correspond to popular head items.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with the following columns: col_item, item_novelty.\n",
        "    \"\"\"\n",
        "\n",
        "    n_records = train_df.shape[0]\n",
        "    item_count = pd.DataFrame(\n",
        "        {\"count\": train_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    item_count[\"item_novelty\"] = -np.log2(item_count[\"count\"] / n_records)\n",
        "    df_item_novelty = (\n",
        "        item_count[[col_item, \"item_novelty\"]]\n",
        "        .sort_values(col_item)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_item_novelty\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def novelty(train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL):\n",
        "    \"\"\"Calculate the average novelty in a list of recommended items (this assumes that the recommendation list\n",
        "    is already computed). Follows section 5 from\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:\n",
        "        choice, discovery and relevance, ECIR 2011\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "\n",
        "    Returns:\n",
        "        float: novelty.\n",
        "    \"\"\"\n",
        "\n",
        "    df_item_novelty = historical_item_novelty(train_df, reco_df, col_user, col_item)\n",
        "    n_recommendations = reco_df.shape[0]\n",
        "    reco_item_count = pd.DataFrame(\n",
        "        {\"count\": reco_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    reco_item_novelty = reco_item_count.merge(df_item_novelty, on=col_item)\n",
        "    reco_item_novelty[\"product\"] = (\n",
        "        reco_item_novelty[\"count\"] * reco_item_novelty[\"item_novelty\"]\n",
        "    )\n",
        "    avg_novelty = reco_item_novelty.agg({\"product\": \"sum\"})[0] / n_recommendations\n",
        "\n",
        "    return avg_novelty\n",
        "\n",
        "\n",
        "\n",
        "# Serendipity metrics\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "@lru_cache_df(maxsize=1)\n",
        "def user_item_serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate serendipity of each item in the recommendations for each user.\n",
        "    The metric definition is based on the following references:\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "    Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:\n",
        "    introducing serendipity into music recommendation, WSDM 2012\n",
        "\n",
        "    Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,\n",
        "    eugeneyan.com, April 2020\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with columns: col_user, col_item, user_item_serendipity.\n",
        "    \"\"\"\n",
        "    # for every col_user, col_item in reco_df, join all interacted items from train_df.\n",
        "    # These interacted items are repeated for each item in reco_df for a specific user.\n",
        "    df_cosine_similarity = _get_cosine_similarity(\n",
        "        train_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    reco_user_item = reco_df[[col_user, col_item]]\n",
        "    reco_user_item[\"reco_item_tmp\"] = reco_user_item[col_item]\n",
        "\n",
        "    train_user_item = train_df[[col_user, col_item]]\n",
        "    train_user_item.columns = [col_user, \"train_item_tmp\"]\n",
        "\n",
        "    reco_train_user_item = reco_user_item.merge(train_user_item, on=[col_user])\n",
        "    reco_train_user_item[\"i1\"] = reco_train_user_item[\n",
        "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
        "    ].min(axis=1)\n",
        "    reco_train_user_item[\"i2\"] = reco_train_user_item[\n",
        "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
        "    ].max(axis=1)\n",
        "\n",
        "    reco_train_user_item_sim = reco_train_user_item.merge(\n",
        "        df_cosine_similarity, on=[\"i1\", \"i2\"], how=\"left\"\n",
        "    )\n",
        "    reco_train_user_item_sim[col_sim].fillna(0, inplace=True)\n",
        "\n",
        "    reco_user_item_avg_sim = (\n",
        "        reco_train_user_item_sim.groupby([col_user, col_item])\n",
        "        .agg({col_sim: \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "    reco_user_item_avg_sim.columns = [\n",
        "        col_user,\n",
        "        col_item,\n",
        "        \"avg_item2interactedHistory_sim\",\n",
        "    ]\n",
        "\n",
        "    df_user_item_serendipity = reco_user_item_avg_sim.merge(\n",
        "        reco_df, on=[col_user, col_item]\n",
        "    )\n",
        "    df_user_item_serendipity[\"user_item_serendipity\"] = (\n",
        "        1 - df_user_item_serendipity[\"avg_item2interactedHistory_sim\"]\n",
        "    ) * df_user_item_serendipity[col_relevance]\n",
        "    df_user_item_serendipity = (\n",
        "        df_user_item_serendipity[[col_user, col_item, \"user_item_serendipity\"]]\n",
        "        .sort_values([col_user, col_item])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_user_item_serendipity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def user_serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average serendipity for each user's recommendations.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with following columns: col_user, user_serendipity.\n",
        "    \"\"\"\n",
        "    df_user_item_serendipity = user_item_serendipity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "        col_relevance,\n",
        "    )\n",
        "    df_user_serendipity = (\n",
        "        df_user_item_serendipity.groupby(col_user)\n",
        "        .agg({\"user_item_serendipity\": \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "    df_user_serendipity.columns = [col_user, \"user_serendipity\"]\n",
        "    df_user_serendipity = df_user_serendipity.sort_values(col_user).reset_index(\n",
        "        drop=True\n",
        "    )\n",
        "\n",
        "    return df_user_serendipity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average serendipity for recommendations across all users.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        float: serendipity.\n",
        "    \"\"\"\n",
        "    df_user_serendipity = user_serendipity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "        col_relevance,\n",
        "    )\n",
        "    avg_serendipity = df_user_serendipity.agg({\"user_serendipity\": \"mean\"})[0]\n",
        "    return avg_serendipity\n",
        "\n",
        "\n",
        "\n",
        "# Coverage metrics\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def catalog_coverage(\n",
        "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
        "):\n",
        "    \"\"\"Calculate catalog coverage for recommendations across all users.\n",
        "    The metric definition is based on the \"catalog coverage\" definition in the following reference:\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        G. Shani and A. Gunawardana, Evaluating Recommendation Systems,\n",
        "        Recommender Systems Handbook pp. 257-297, 2010.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "\n",
        "    Returns:\n",
        "        float: catalog coverage\n",
        "    \"\"\"\n",
        "    # distinct item count in reco_df\n",
        "    count_distinct_item_reco = reco_df[col_item].nunique()\n",
        "    # distinct item count in train_df\n",
        "    count_distinct_item_train = train_df[col_item].nunique()\n",
        "\n",
        "    # catalog coverage\n",
        "    c_coverage = count_distinct_item_reco / count_distinct_item_train\n",
        "    return c_coverage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def distributional_coverage(\n",
        "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
        "):\n",
        "    \"\"\"Calculate distributional coverage for recommendations across all users.\n",
        "    The metric definition is based on formula (21) in the following reference:\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        G. Shani and A. Gunawardana, Evaluating Recommendation Systems,\n",
        "        Recommender Systems Handbook pp. 257-297, 2010.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "\n",
        "    Returns:\n",
        "        float: distributional coverage\n",
        "    \"\"\"\n",
        "    # In reco_df, how  many times each col_item is being recommended\n",
        "    df_itemcnt_reco = pd.DataFrame(\n",
        "        {\"count\": reco_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "\n",
        "    # the number of total recommendations\n",
        "    count_row_reco = reco_df.shape[0]\n",
        "\n",
        "    df_entropy = df_itemcnt_reco\n",
        "    df_entropy[\"p(i)\"] = df_entropy[\"count\"] / count_row_reco\n",
        "    df_entropy[\"entropy(i)\"] = df_entropy[\"p(i)\"] * np.log2(df_entropy[\"p(i)\"])\n",
        "\n",
        "    d_coverage = -df_entropy.agg({\"entropy(i)\": \"sum\"})[0]\n",
        "\n",
        "    return d_coverage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LB-Pqam_pWD",
        "outputId": "1240c907-e781-4fab-9190-84372c821e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gcvVmDGmGUGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ull0GOIZ9j0P"
      },
      "outputs": [],
      "source": [
        "quora_engine = CoreEngine(25, 0.9,\n",
        "                              '/content/drive/MyDrive/quora_docs.csv',\n",
        "                              '/content/drive/MyDrive/quora_queries.csv',\n",
        "                              '/content/drive/MyDrive/quora_grels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sadTYW-JFPbx"
      },
      "outputs": [],
      "source": [
        "lotte_engine = CoreEngine(50, 0.5,\n",
        "                              '/content/drive/MyDrive/lotte_docs.csv',\n",
        "                              '/content/drive/MyDrive/lotte_queries.csv',\n",
        "                              '/content/drive/MyDrive/lotte_grels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wETGo1AXFbtn"
      },
      "outputs": [],
      "source": [
        "eva_quora = Evaluation(quora_engine)\n",
        "eva_quora.init_queries_array()\n",
        "eva_quora.init_predection_array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fdkMgL-bFVpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa597d23-4afb-4c59-be9b-a6b66910705a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision_at_k': 0.08806000000000001,\n",
              " 'recall_at_k': 0.6666661277382748,\n",
              " 'map_at_k': 0.5111678643235071,\n",
              " 'mean_reciprocal_rank': 0.5522134126984127}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "measurements_quora = eva_quora.measurements()\n",
        "measurements_quora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sxryIa0MS85g"
      },
      "outputs": [],
      "source": [
        "eva_lotte = Evaluation(lotte_engine)\n",
        "eva_lotte.init_queries_array()\n",
        "eva_lotte.init_predection_array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLPeKg16S-nf",
        "outputId": "374b4687-bcf6-494e-bae1-d9467124d356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision_at_k': 0.05563835072031794,\n",
              " 'recall_at_k': 0.10805744847416572,\n",
              " 'map_at_k': 0.05698679631463264,\n",
              " 'mean_reciprocal_rank': 0.17282623266229824}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "measurements_lotte = eva_lotte.measurements ()\n",
        "measurements_lotte"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}